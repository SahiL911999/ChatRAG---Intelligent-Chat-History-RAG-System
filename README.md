# ü§ñ ChatRAG - Intelligent Chat History RAG System

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://langchain.com)
[![Pinecone](https://img.shields.io/badge/Pinecone-Vector%20DB-orange.svg)](https://pinecone.io)
[![AWS](https://img.shields.io/badge/AWS-S3%20%7C%20Lambda-yellow.svg)](https://aws.amazon.com)

> **A sophisticated Retrieval-Augmented Generation (RAG) system that processes chat conversations, classifies them using AWS Lambda, and enables intelligent search with citation support.**

## üë®‚Äçüíª Author
**Sahil Ranmbail** - *AI Engineer*

---

## üìã Table of Contents
- [üéØ Overview](#-overview)
- [üèóÔ∏è Architecture](#Ô∏è-architecture)
- [‚ú® Features](#-features)
- [üîß Installation](#-installation)
- [‚öôÔ∏è Configuration](#Ô∏è-configuration)
- [üöÄ Usage](#-usage)
- [üìä Pipeline Workflow](#-pipeline-workflow)
- [üîç Search & Query](#-search--query)
- [üìÅ Project Structure](#-project-structure)
- [ü§ù Contributing](#-contributing)

---

## üéØ Overview

ChatRAG is an end-to-end RAG system designed to:
- üì• **Ingest** chat conversations from S3 storage
- üß† **Classify** conversations using AWS Lambda (work/personal)
- ‚ö° **Process** and chunk text for optimal retrieval
- üîç **Store** embeddings in Pinecone vector database
- üí¨ **Enable** intelligent search with proper citations

---

## üèóÔ∏è Architecture

```mermaid
graph TB
    A[üìÅ S3 Chat JSON] --> B[üîÑ Complete Pipeline]
    B --> C[‚ö° AWS Lambda Classification]
    C --> D[üìä Accessibility Scoring]
    D --> E[üìù Document Processing]
    E --> F[‚úÇÔ∏è Text Chunking]
    F --> G[üßÆ Google Embeddings]
    G --> H[üóÑÔ∏è Pinecone Vector Store]
    
    I[‚ùì User Query] --> J[üîç Search Wrapper]
    J --> K[üßÆ Query Embedding]
    K --> L[üéØ Vector Similarity Search]
    L --> M[ü§ñ Groq LLM Generation]
    M --> N[üìö Response with Citations]
    
    H -.-> L
    
    style A fill:#e1f5fe
    style H fill:#f3e5f5
    style N fill:#e8f5e8
    style C fill:#fff3e0
```

---

## ‚ú® Features

### üîÑ **Data Pipeline**
- **S3 Integration**: Direct processing from S3 URIs
- **Lambda Classification**: Automatic work/personal categorization
- **Smart Chunking**: Optimized text splitting for better retrieval
- **Metadata Preservation**: Complete conversation context retention

### üîç **Search & Retrieval**
- **Vector Similarity**: Semantic search using Google embeddings
- **User Filtering**: Search within specific user's conversations
- **Citation Support**: Automatic source referencing
- **Confidence Scoring**: Accessibility classification confidence

### ü§ñ **AI Integration**
- **Google Embeddings**: `text-embedding-004` model
- **Groq LLM**: High-performance language model
- **Pinecone**: Serverless vector database
- **LangChain**: Orchestration framework

---

## üîß Installation

### Prerequisites
- üêç Python 3.8+
- üîë AWS Account with S3 access
- üå≤ Pinecone account
- üîê Google AI API key
- ‚ö° Groq API key

### Setup Steps

1. **Clone the repository**
```bash
git clone https://github.com/SahiL911999/ChatRAG---Intelligent-Chat-History-RAG-System.git
cd ChatRAG---Intelligent-Chat-History-RAG-System
```

2. **Create virtual environment**
```bash
python -m venv virenv
# Windows
virenv\Scripts\activate
# Linux/Mac
source virenv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your API keys
```

---

## ‚öôÔ∏è Configuration

Create a `.env` file with the following variables:

```env
# üîê API Keys
GOOGLE_API_KEY=your_google_api_key
PINECONE_API_KEY=your_pinecone_api_key
GROQ_API_KEY=your_groq_api_key

# üë§ User Configuration
CHAT_USER=Your Name
CHAT_ENGINE=ChatGPT

# ‚òÅÔ∏è AWS Configuration
bucket_name=your-s3-bucket
folder_name=your-folder-name
region=us-east-1

# üóÑÔ∏è Pinecone Configuration
PINECONE_INDEX=your-index-name
Dimension=768
```

---

## üöÄ Usage

### üì• **Data Ingestion**

```python
from complete_pipeline import s3_json_load_ingest

# Define Lambda event for classification
test_event = {
    "category_one": "personal",
    "category_two": "work", 
    "file_path": "s3://your-bucket/path/to/chat.json"
}

# Process and ingest data
s3_json_load_ingest(
    s3_uri=test_event["file_path"], 
    lambda_event=test_event
)
```

### üîç **Search & Query**

```python
from search_wrapper import RAGCitationEngine

# Initialize the search engine
rag_engine = RAGCitationEngine()

# Query with citations
result = rag_engine.query(
    query="How to fix Windows settings?",
    chat_user="sahil Ranmbail"  # Optional: filter by user
)

print("Answer:", result["answer"])
print("References:", result["references"])
```

---

## üìä Pipeline Workflow

### üîÑ **Complete Pipeline Process**

```
üìÅ S3 JSON File
    ‚Üì
‚ö° AWS Lambda Classification
    ‚Üì (Confidence Scoring)
üìä Accessibility Determination
    ‚Üì
üìù Document Creation
    ‚Üì
‚úÇÔ∏è Text Chunking (150 chars, 30 overlap)
    ‚Üì
üßÆ Google Embeddings Generation
    ‚Üì
üóÑÔ∏è Pinecone Vector Storage
```

### üéØ **Classification Logic**
- **Work**: `category_two.probability >= 0.9`
- **Personal**: `category_two.probability < 0.9`
- **Confidence Score**: Stored with each document

### üìù **Document Structure**
```json
{
  "page_content": "Chat message content",
  "metadata": {
    "chat_engine": "ChatGPT",
    "chat_account": "User Name",
    "chat_id": "unique-chat-id",
    "title": "Conversation Title",
    "accessibility": "work|personal",
    "accessibility_confidence_score": 0.95,
    "chunk_id": "chat_id::turn_id::chunk_index"
  }
}
```

---

## üîç Search & Query

### üéØ **Search Features**
- **Semantic Search**: Vector similarity matching
- **User Filtering**: Search specific user's chats
- **Citation Tracking**: Automatic source referencing
- **Metadata Rich**: Full conversation context

### üìö **Citation Format**
```python
{
  "answer": "Windows settings can be reset through... [1]",
  "references": [
    {
      "source_id": "[1]",
      "title": "Windows Settings Fix",
      "chat_id": "abc-123",
      "turn_id": "def-456", 
      "timestamp": "2025-01-15 10:30:00"
    }
  ]
}
```

---

## üìÅ Project Structure

```
chatrag/
‚îú‚îÄ‚îÄ üìÑ complete_pipeline.py    # Main ingestion pipeline
‚îú‚îÄ‚îÄ üîç search_wrapper.py       # Search & query engine
‚îú‚îÄ‚îÄ üìã requirements.txt        # Dependencies
‚îú‚îÄ‚îÄ ‚öôÔ∏è .env                   # Environment variables
‚îú‚îÄ‚îÄ üìö README.md              # This file
‚îú‚îÄ‚îÄ üö´ .gitignore             # Git ignore rules
‚îî‚îÄ‚îÄ üìÅ virenv/                # Virtual environment
```

### üóÇÔ∏è **Core Files**

| File | Purpose | Key Features |
|------|---------|--------------|
| `complete_pipeline.py` | üì• Data ingestion | S3 ‚Üí Lambda ‚Üí Pinecone |
| `search_wrapper.py` | üîç Search engine | RAG + Citations |
| `requirements.txt` | üì¶ Dependencies | All required packages |
| `.env` | ‚öôÔ∏è Configuration | API keys & settings |

---

## üîß Advanced Configuration

### üéõÔ∏è **Chunking Parameters**
```python
# Adjust in complete_pipeline.py
splitter = RecursiveCharacterTextSplitter(
    chunk_size=150,      # Optimal for chat messages
    chunk_overlap=30,    # Context preservation
    separators=["\n\n", "\n", " ", ""]
)
```

### üéØ **Search Parameters**
```python
# Adjust in search_wrapper.py
def query(self, query: str, chat_user: str = "", k: int = 10):
    # k = number of similar documents to retrieve
```

---

## üö® Troubleshooting

### Common Issues

**üîë Authentication Errors**
- Verify all API keys in `.env`
- Check AWS credentials configuration

**üìä Empty Results**
- Ensure Pinecone index exists
- Verify data ingestion completed successfully

**üîç Search Issues**
- Check user filter spelling
- Verify embeddings model consistency

---

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **LangChain** for the RAG framework
- **Pinecone** for vector database services
- **Google AI** for embedding models
- **Groq** for high-performance LLM inference
- **AWS** for cloud infrastructure

---

<div align="center">

**Built with ‚ù§Ô∏è by Sahil Ranmbail**

[![GitHub](https://img.shields.io/badge/GitHub-Follow-black.svg)](https://github.com/sahilranmbail)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue.svg)](https://linkedin.com/in/sahilranmbail)

</div>